# 协程的使用
- 背景：python3.6 + flask + gevent + celery + requests
- 场景：celery定时去pdd同步我们的订单信息，有时候订单会非常多，且api限制一次只能拿100条订单，所以执行一次任务可能会调用1000+次api
  ```
  import requests
  import json
  
  order_api = ''
  
  
  def get_page_order(page):
      ret = requests.get(order_api + '?page=%s' % page)
      return json.loads(ret.text).get('order_list') 
  
  
  if __name__ == '__main__':
      order_count = 100000
      page_num = order_count / 100
      order_list = []
      for page in range(0, page_num):
          order_list.extend(get_page_order(page))
  ```
- 问题：目前是直接采用同步方式按顺序一个个调用api，效率非常低下，需要较长时间才能同步完。本次测试结果为106s。
- 解决:
    - 方法1: 多线程
    
      众所周知,cpython是有GIL锁的，对多线程并不友好，但并不意味着编写多线程Python程序没有价值。
      如果应用程序受CPU限制并且大部分时间都在执行CPU密集型计算，那么GIL会对多线程性能产生负面影响。
      许多应用程序受I/O密集型的，它们主要等待外部操作，如网络操作或用户输入，所以可以通过使用Python的线程模块实现多个并发线程来获得并行加速。
      ```
      from concurrent.futures import ALL_COMPLETED
      from concurrent.futures.thread import ThreadPoolExecutor
      
      # 使用线程池，防止线程过多导致不断切换线程而额外的开销。
      with ThreadPoolExecutor() as t:  # 此处不设置max_workers，则会默认为最大[(os.cpu_count() or 1) * 5]个线程
          all_task = [t.submit(get_page_order, page) for page in range(0, page_num)]
          wait(all_task, return_when=ALL_COMPLETED)
          for task in all_task:
              order_list.extend(task.result())
      return order_list
      ```
      经测试，运行时间为15.76s，效率明显提升，但实际上运行过程中还有部分时间cpu处于停滞状态，没有100%的利用上。
      比如一个2核CPU服务器，max_workers = 10，同时运行10个线程满了之后，且10个请求都还没返回数据时，只能继续停滞等待，直到某个线程返回数据才能继续执行下一个请求。
     - 方法2: 多进程
     
        pass
     
     - 方法3: 协程
     
     协程实质是只有一个线程在执行，由于由程序主动控制切换，没有线程切换的开销，所以执行效率极高。对于IO密集型任务非常适用，如果是cpu密集型，推荐多进程+协程的方式。
     由于requests不是一个异步的请求库，即使封装一个coroutine函数里用requests做网络请求，依然会阻塞，所以首先要把网络库改为异步网络库，这里使用了aiohttp。
     ```
     from aiohttp import ClientSession
     async def get_page_order(page):
         async with ClientSession() as session:
             async with session.get(order_api + '?page=%s' % page) as response:
                 response = await response.read()
        return json.loads(response).get('order_list') 
        
        
    if __name__ == '__main__':
        order_count = 100000
        page_num = order_count / 100
        order_list = [] 
        loop = asyncio.get_event_loop()
        tasks = [loop.create_task(get_page_order(page)) for page in range(0, page_num)]
        for task in tasks:
            loop.run_until_complete(task)
        loop.close()
        for task in tasks:
            order_list.extend(task.result())
     ```
     经测试，运行时间为5.98s
- 结果： 协程完胜
     
     
     
     
